- name: Ensure Rook Helm Repo exists
  kubernetes.core.helm_repository:
    name: rook-release
    repo_url: https://charts.rook.io/release

- name: Ensure rook-operator is installed
  kubernetes.core.helm:
    name: rook-ceph
    namespace: rook-ceph
    create_namespace: true
    chart_ref: rook-release/rook-ceph
    chart_version: v1.10.8
    values:
      csi.kubeletDirPath: /var/lib/k0s/kubelet

- name: Ensure Ceph Cluster is installed
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephCluster
      metadata:
        name: rook-ceph
        namespace: rook-ceph # namespace:cluster
      spec:
        cephVersion:
          image: quay.io/ceph/ceph:v17.2.5
        dataDirHostPath: /var/lib/rook
        mon:
          count: 3
          allowMultiplePerNode: false
        mgr:
          count: 2
          modules:
            - name: pg_autoscaler
              enabled: true
        dashboard:
          enabled: true
          # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
          # urlPrefix: /ceph-dashboard
          # serve the dashboard at the given port.
          # port: 8443
          # serve the dashboard using SSL
          ssl: true
        # enable prometheus alerting for cluster
        monitoring:
          # requires Prometheus to be pre-installed
          enabled: false
        network:
          connections:
            encryption:
              enabled: false
            compression:
              enabled: false
            # enable host networking
            #provider: host
            # enable the Multus network provider
            #provider: multus
            #selectors:
            # The selector keys are required to be `public` and `cluster`.
            # Based on the configuration, the operator will do the following:
            #   1. if only the `public` selector key is specified both public_network and cluster_network Ceph settings will listen on that interface
            #   2. if both `public` and `cluster` selector keys are specified the first one will point to 'public_network' flag and the second one to 'cluster_network'
            #
            # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
            #
            #public: public-conf --> NetworkAttachmentDefinition object name in Multus
            #cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
          # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
          #ipFamily: "IPv6"
          # Ceph daemons to listen on both IPv4 and Ipv6 networks
          #dualStack: false
        # enable the crash collector for ceph daemon crash collection
        crashCollector:
          disable: false
          # Uncomment daysToRetain to prune ceph crash entries older than the
          # specified number of days.
          #daysToRetain: 30
        # enable log collector, daemons will log on files and rotate
        logCollector:
          enabled: true
          periodicity: daily # one of: hourly, daily, weekly, monthly
          maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1M.
        # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
        cleanupPolicy:
          # Since cluster cleanup is destructive to data, confirmation is required.
          # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
          # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
          # Rook will immediately stop configuring the cluster and only wait for the delete command.
          # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
          confirmation: ""
          # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
          sanitizeDisks:
            # method indicates if the entire disk should be sanitized or simply ceph's metadata
            # in both case, re-install is possible
            # possible choices are 'complete' or 'quick' (default)
            method: quick
            # dataSource indicate where to get random bytes from to write on the disk
            # possible choices are 'zero' (default) or 'random'
            # using random sources will consume entropy from the system and will take much more time then the zero source
            dataSource: zero
            # iteration overwrite N times instead of the default (1)
            # takes an integer value
            iteration: 1
          # allowUninstallWithVolumes defines how the uninstall should be performed
          # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
          allowUninstallWithVolumes: false
        # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
        # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=provision-storage-node' and
        # tolerate taints with a key of 'provision-storage-node'.
        #  placement:
        #    all:
        #      nodeAffinity:
        #        requiredDuringSchedulingIgnoredDuringExecution:
        #          nodeSelectorTerms:
        #          - matchExpressions:
        #            - key: role
        #              operator: In
        #              values:
        #              - provision-storage-node
        #      podAffinity:
        #      podAntiAffinity:
        #      topologySpreadConstraints:
        #      tolerations:
        #      - key: provision-storage-node
        #        operator: Exists
        # The above placement information can also be specified for mon, osd, and mgr components
        #    mon:
        # Monitor deployments may contain an anti-affinity rule for avoiding monitor
        # collocation on the same node. This is a required rule when host network is used
        # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
        # preferred rule with weight: 50.
        #    osd:
        #    prepareosd:
        #    mgr:
        #    cleanup:
        annotations:
        #    all:
        #    mon:
        #    osd:
        #    cleanup:
        #    prepareosd:
        # clusterMetadata annotations will be applied to only `rook-ceph-mon-endpoints` configmap and the `rook-ceph-mon` and `rook-ceph-admin-keyring` secrets.
        # And clusterMetadata annotations will not be merged with `all` annotations.
        #    clusterMetadata:
        #       kubed.appscode.com/sync: "true"
        # If no mgr annotations are set, prometheus scrape annotations will be set by default.
        #    mgr:
        labels:
        #    all:
        #    mon:
        #    osd:
        #    cleanup:
        #    mgr:
        #    prepareosd:
        # monitoring is a list of key-value pairs. It is injected into all the monitoring resources created by operator.
        # These labels can be passed as LabelSelector to Prometheus
        #    monitoring:
        #    crashcollector:
        resources:
        # The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
        #    mgr:
        #      limits:
        #        cpu: "500m"
        #        memory: "1024Mi"
        #      requests:
        #        cpu: "500m"
        #        memory: "1024Mi"
        # The above example requests/limits can also be added to the other components
        #    mon:
        #    osd:
        # For OSD it also is a possible to specify requests/limits based on device class
        #    osd-hdd:
        #    osd-ssd:
        #    osd-nvme:
        #    prepareosd:
        #    mgr-sidecar:
        #    crashcollector:
        #    logcollector:
        #    cleanup:
        # The option to automatically remove OSDs that are out and are safe to destroy.
        removeOSDsIfOutAndSafeToRemove: false
        priorityClassNames:
          #all: rook-ceph-default-priority-class
          mon: system-node-critical
          osd: system-node-critical
          mgr: system-cluster-critical
          #crashcollector: rook-ceph-crashcollector-priority-class
        storage: # cluster level provision-storage configuration and selection
          useAllNodes: true
          useAllDevices: true
          #deviceFilter:
          config:
          # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
          # metadataDevice: "md0" # specify a non-rotational provision-storage so ceph-volume will use it as block db device of bluestore.
          # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
          # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
          # osdsPerDevice: "1" # this value can be overridden at the node or device level
          # encryptedDevice: "true" # the default value for this option is "false"
          # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
          # nodes below will be used as provision-storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
          # nodes:
          #   - name: "172.17.4.201"
          #     devices: # specific devices to use for provision-storage can be specified for each node
          #       - name: "sdb"
          #       - name: "nvme01" # multiple osds can be created on high performance devices
          #         config:
          #           osdsPerDevice: "5"
          #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
          #     config: # configuration can be specified at the node level which overrides the cluster level config
          #   - name: "172.17.4.301"
          #     deviceFilter: "^sd."
          # when onlyApplyOSDPlacement is false, will merge both placement.All() and placement.osd
          onlyApplyOSDPlacement: false
        # The section for configuring management of daemon disruptions during upgrade or fencing.
        disruptionManagement:
          # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
          # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
          # block eviction of OSDs by default and unblock them safely when drains are detected.
          managePodBudgets: true
          # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
          # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
          osdMaintenanceTimeout: 30
          # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
          # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
          # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
          pgHealthCheckTimeout: 0
          # If true, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy.
          # Only available on OpenShift.
          manageMachineDisruptionBudgets: false
          # Namespace in which to watch for the MachineDisruptionBudgets.
          machineDisruptionBudgetNamespace: openshift-machine-api

        # healthChecks
        # Valid values for daemons are 'mon', 'osd', 'status'
        healthCheck:
          daemonHealth:
            mon:
              disabled: false
              interval: 45s
            osd:
              disabled: false
              interval: 60s
            status:
              disabled: false
              interval: 60s
          # Change pod liveness probe timing or threshold values. Works for all mon,mgr,osd daemons.
          livenessProbe:
            mon:
              disabled: false
            mgr:
              disabled: false
            osd:
              disabled: false
          # Change pod startup probe timing or threshold values. Works for all mon,mgr,osd daemons.
          startupProbe:
            mon:
              disabled: false
            mgr:
              disabled: false
            osd:
              disabled: false
